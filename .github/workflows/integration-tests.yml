name: Integration tests

on: [push]

jobs:
  ollama:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          python-version: "3.12"

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Pull Ollama image
        run: |
          ollama pull llama3.2:3b-instruct-fp16

      - name: Start Ollama in Background
        run: |
          nohup ollama run llama3.2:3b-instruct-fp16 > ollama.log 2>&1 &

      - name: Wait for Ollama to Start
        run: |
          echo "Waiting for Ollama..."
          for i in {1..30}; do
            if curl -s http://localhost:11434 | grep -q "Ollama is running"; then
              echo "Ollama is running!"
              exit 0
            fi
            sleep 1
          done
          echo "Ollama failed to start"
          ollama ps
          exit 1

      - name: Set Up Environment and Install Dependencies
        run: |
          uv sync --extra dev --extra test
          uv pip install ollama faiss-cpu

      - name: Start Llama Server in Background
        env:
          INFERENCE_MODEL: "meta-llama/Llama-3.2-3B-Instruct"
        run: |
          source .venv/bin/activate
          nohup uv run --extra dev --extra test python -m llama_stack.distribution.server.server --yaml-config ./llama_stack/templates/ollama/run.yaml > server.log 2>&1 &

      - name: Wait for Server to be Ready
        run: |
          echo "Waiting for server..."
          for i in {1..30}; do
            if curl -s http://localhost:8321/v1/health | grep -q "OK"; then
              echo "Server is up!"
              exit 0
            fi
            sleep 1
          done
          echo "Server failed to start"
          ps fauxw
          cat server.log
          exit 1  # Fail if the server never starts

      - name: Run Inference Integration Tests
        env:
          INFERENCE_MODEL: "meta-llama/Llama-3.2-3B-Instruct"
        run: |
          uv run --extra dev --extra test -- pytest --stack-config=ollama --text-model="meta-llama/Llama-3.2-3B-Instruct" -v --record-responses tests/integration/inference
